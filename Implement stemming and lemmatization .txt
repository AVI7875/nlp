CODE / OUTPUT : -


import nltk import nltk.corpus
from nltk.tokenize import word_tokenize

import nltk nltk.download('wordnet')
[nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data]	Unzipping corpora/wordnet.zip.

True
mystring="We are msc students, we likes to talk" import nltk
nltk.download('punkt')

[nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data]	Unzipping tokenizers/punkt.zip.
True mytokens=word_tokenize(mystring) import nltk
nltk.download('averaged_perceptron_tagger')
[nltk_data] Downloading package averaged_perceptron_tagger to [nltk_data]	/root/nltk_data...
[nltk_data]	Unzipping taggers/averaged_perceptron_tagger.zip.

True

for i in mytokens: print(nltk.pos_tag([i]))

-------------

#Applying Named Entity Recognition

String="Jay and Viru Stays in India, He is working in Syntel Ltd Company"

Apply Tokenizer -> POs tag -> NER
String_Sample=word_tokenize(String) String_Sample

String_tag=nltk.pos_tag(String_Sample)

for i in String_Sample: print(nltk.pos_tag([i]))

--------------

import nltk nltk.download('maxent_ne_chunker')

-------

import nltk
nltk.download('words')

------------

from nltk.chunk import ne_chunk NET=ne_chunk(String_tag) print(NET)

----------

String1="Merry lives in India"
String1_token=word_tokenize(String1) import nltk
nltk.download('punkt'

String1_tags=nltk.pos_tag(String1_token)
String1_tags

String1_ner=ne_chunk(String1_tags) print(String1_ner)

import nltk nltk.download('averaged_perceptron_tagger') import nltk nltk.download('maxent_ne_chunker')
import nltk nltk.download('words')

------------------

#Spacy: C + Python

import spacy nlp=spacy.load('en_core_web_sm')

doc=nlp("This is Python with nlp Practicals")
for token in doc: print(token.text)

-------------

#Python with nlp 

doc=nlp("This is Python!")
for token in doc:
print(token.i, token.text, token.pos_)

--------

doc=nlp("Joy doesn't share lunch, He buy U.k, Startup for 2$ million")

for ent in doc.ents: print(ent.text, ent.label_)


------------

from spacy.matcher import Matcher
doc=nlp("Barak Obama is a former president of United States and He's vacat ing white house")
pattern = [{'LEMMA':'vacate'}, {'ORTH':'white'}] matcher = Matcher(nlp.vocab) matcher.add('white_Pattern', None, pattern)
matches = matcher(doc) print(matches)
for match_id, start, end in matches: matched_span=doc[start:end] 
print(matched_span.text)


